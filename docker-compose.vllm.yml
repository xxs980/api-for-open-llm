version: '3.10'

services:
  #公共配置
  
  common:
    image: llm-api:vllm2
    command: python api/vllm_server.py
    shm_size: '60gb'
    ulimits:
      stack: 67108864
      memlock: -1
    volumes:
      - /mnt/data1/llm_models:/workspace/llm_models
      - /home/zhoupengzhen/api-for-open-llm/api:/workspace/api
      - /home/zhoupengzhen/data/checkpoints:/workspace/checkpoints
    env_file:
      - .env.vllm.example

  qwen:
    extends:
        service: common
    environment:
      - PORT=8001
      - HOST=0.0.0.0
      - MODEL_NAME=qwen
      - MODEL_PATH=llm_models/Qwen-7B-Chat
      - EMBEDDING_NAME=checkpoints/m3e-base
      - DEVICE_MAP=auto

    ports:
      - "192.168.10.254:6001:8001"
     
    restart: always
    networks:
      - vllmapinet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  baichuan:
    extends:
        service: common
    environment:
      - PORT=8002
      - HOST=0.0.0.0
      - MODEL_NAME=baichuan
      - MODEL_PATH=llm_models/Baichuan-13B-Chat
      - EMBEDDING_NAME=checkpoints/m3e-base
      - DEVICE_MAP=auto
    ports:
      - "192.168.10.254:6002:8002"
    restart: always
    networks:
      - vllmapinet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1,2,3']
              capabilities: [gpu]

  chatglm2:
    extends:
        service: common
    environment:
      - PORT=8003
      - HOST=0.0.0.0
      - MODEL_NAME=chatglm2
      - MODEL_PATH=llm_models/chatglm2-6b
      - EMBEDDING_NAME=checkpoints/m3e-base
      - DEVICE_MAP=auto
    ports:
      - "192.168.10.254:6003:8003"
      
    restart: always
    networks:
      - vllmapinet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['4']
              capabilities: [gpu]

  internlm:
      extends:
          service: common
      environment:
        - PORT=8004
        - HOST=0.0.0.0
        - MODEL_NAME=internlm
        - MODEL_PATH=llm_models/internlm-chat-7b-v1_1
        - EMBEDDING_NAME=checkpoints/m3e-base
        - DEVICE_MAP=auto
      ports:
        - "192.168.10.254:6004:8004"
      
      restart: always
      networks:
        - vllmapinet
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                device_ids: ['1','2']
                capabilities: [gpu]

  code-llama:
    extends:
        service: common
    environment:
      - PORT=8005
      - HOST=0.0.0.0
      - MODEL_NAME=code-llama
      - MODEL_PATH=llm_models/llama-2-13b-chat
      - EMBEDDING_NAME=checkpoints/m3e-base
      - DEVICE_MAP=auto
    ports:
      - "192.168.10.254:6005:8005"
    
    restart: always
    networks:
      - vllmapinet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3','4']
              capabilities: [gpu]

networks:
  vllmapinet:
    driver: bridge
    name: vllmapinet